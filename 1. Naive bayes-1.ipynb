{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1add301-0bd9-4a20-9151-96ba70455296",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6dd78-7d89-4f03-8cf6-746060be6881",
   "metadata": {},
   "source": [
    "Bayes’ theorem describes the probability of occurrence of an event related to any condition. It is also considered for the case of conditional probability. Bayes theorem is also known as the formula for the probability of “causes”.\n",
    "\n",
    "For example: if we have to calculate the probability of taking a blue ball from the second bag out of three different bags of balls, where each bag contains three different colour balls viz. red, blue, black. In this case, the probability of occurrence of an event is calculated depending on other conditions is known as conditional probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ff40f-fb93-47b9-96f2-936714c6b69d",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d230b-54e3-4536-957c-511ea9480436",
   "metadata": {},
   "source": [
    "If A and B are two events, then the formula for the Bayes theorem is given by:\n",
    "\n",
    "P(A|B) = [P(A) * P(B|A)] / P(B)\n",
    "\n",
    "- P(A∣B) represents the conditional probability of event A occurring given that event B has occurred.\n",
    "- P(B∣A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "- P(A) is the prior probability of event A, which is our initial belief in the probability of A happening.\n",
    "- P(B) is the prior probability of event B, which is our initial belief in the probability of B happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ebe89-0cc0-4bac-97c9-3fa9f7710ffa",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcc959-b352-4cba-a712-0d107808bb85",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in practice in a wide range of fields and applications to make predictions, perform statistical analysis, and make decisions under uncertainty. Here are some common ways in which Bayes' theorem is applied in practice:\n",
    "\n",
    "1. Medical Diagnosis: Bayes' theorem is used in medical diagnosis to calculate the probability of a patient having a particular disease based on their symptoms and test results. It helps doctors update their beliefs about a patient's condition as new information becomes available.\n",
    "\n",
    "2. Spam Email Filtering: Email spam filters often use Bayes' theorem to classify incoming emails as spam or not spam. The algorithm learns from a dataset of known spam and non-spam emails, updating its probability estimates as it encounters new emails.\n",
    "\n",
    "3. Machine Learning: In machine learning, Bayesian methods are used for probabilistic modeling and inference. Bayesian networks, for example, are graphical models that represent probabilistic relationships among variables and are used for tasks like classification and prediction.\n",
    "\n",
    "4. Natural Language Processing: Bayes' theorem can be used in natural language processing tasks such as text classification and sentiment analysis to estimate the likelihood of a document belonging to a particular category or expressing a certain sentiment.\n",
    "\n",
    "In all these applications, Bayes' theorem provides a framework for updating beliefs or probabilities based on new evidence or data, making it a valuable tool for decision-making in situations where uncertainty and conditional probabilities are important considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbb8e1f-6255-4dee-95fc-2b6b1398f882",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2b7a5-1411-44e7-bbe0-61ed052ba6f4",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability, and it provides a way to calculate conditional probabilities based on prior probabilities and additional evidence. In fact, Bayes' theorem is a formula that explicitly defines how conditional probability is updated when new information becomes available.\n",
    "\n",
    "Here's the relationship between Bayes' theorem and conditional probability:\n",
    "\n",
    "1. Conditional Probability (Prior): Conditional probability represents the probability of an event occurring given that another event has already occurred. It is denoted as P(A|B), where A is the event of interest, and B is the condition or evidence. This represents our prior belief about the probability of A given B.\n",
    "\n",
    "2. Bayes' Theorem: Bayes' theorem is a mathematical formula that relates conditional probabilities. It allows us to update our prior belief about the conditional probability P(A|B) using new evidence. The formula is:\n",
    "\n",
    "   P(A|B) = [P(A) * P(B|A)] / P(B)\n",
    "\n",
    "   - P(A|B) is the updated conditional probability after considering the evidence B.\n",
    "   - P(B|A) is the probability of observing the evidence B given that A is true.\n",
    "   - P(A) is the prior probability of A, representing our initial belief.\n",
    "   - P(B) is the probability of observing the evidence B.\n",
    "\n",
    "3. Updating Conditional Probability: Bayes' theorem tells us how to update our belief in the probability of event A given new evidence B. It takes into account the prior probability P(A), the likelihood P(B|A) of observing B given A, and the overall probability P(B) of observing B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e202f9-711d-42c2-9e28-7690de144c7f",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca226e2-a7df-47bc-b4be-5a841036bcb8",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on the nature of the data and the assumptions that can reasonably be made about the relationships among the features. \n",
    "\n",
    "There are three common types of Naive Bayes classifiers:\n",
    "\n",
    "1. Gaussian Naive Bayes:\n",
    "    - The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\n",
    "    - Example: It is often used in problems involving real-valued attributes, such as predicting the price of a house based on various continuous features like size, number of bedrooms, and location.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "    - The Multinomial Naïve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc. The classifier uses the frequency of words for the predictors.\n",
    "    - Example: Text classification problems like spam email detection, sentiment analysis, or document categorization are typical applications of Multinomial Naive Bayes.\n",
    "    \n",
    "3. Bernoulli Naive Bayes:\n",
    "    - The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks.\n",
    "    - Example: It is often used in problems where the features are binary, such as classifying whether an article contains certain keywords (1 if present, 0 if not) for information retrieval or sentiment analysis tasks.\n",
    "    \n",
    "The choice between these classifiers should be based on the characteristics of your dataset and the assumptions that best fit your problem. Here are some additional considerations:\n",
    "\n",
    "1. Feature Distribution: Assess the distribution of your features. If they closely resemble the assumptions of Gaussian, Multinomial, or Bernoulli distributions, choose the corresponding Naive Bayes classifier.\n",
    "\n",
    "2. Preprocessing: Depending on your dataset, you may need to preprocess your features. For instance, if you have real-valued features but they don't follow a Gaussian distribution, you might consider transforming them (e.g., using log transformations) to make Gaussian Naive Bayes more appropriate.\n",
    "\n",
    "3. Model Performance: Experiment with different Naive Bayes classifiers and evaluate their performance using appropriate metrics (e.g., accuracy, precision, recall, F1-score) and cross-validation techniques. The classifier that performs best on your specific problem should be selected.\n",
    "\n",
    "4. Class Imbalance: Consider the class distribution in your dataset. If you have imbalanced classes, you may need to use techniques like oversampling, undersampling, or adjusting class priors to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7dd6ff-275a-4be9-b085-f8e543722703",
   "metadata": {},
   "source": [
    "Q6. Assignment:\n",
    "\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "  A     3    3    4    4    3    3   3\n",
    "  B     2    2    1    2    2    2   3\n",
    "  \n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394080d-398c-4464-a84f-33381921dd05",
   "metadata": {},
   "source": [
    "For Class A:\n",
    "P(X1=3|A) = 4/10\n",
    "P(X2=4|A) = 3/10\n",
    "\n",
    "For Class B:\n",
    "P(X1=3|B) = 1/9\n",
    "P(X2=4|B) = 3/9\n",
    "\n",
    "Using the Naive Bayes approach, we calculate the probabilities for each class\n",
    "\n",
    "For Class A:\n",
    "\n",
    "P(A|X1=3 , X2=4) = P(X1=3|A) * P(X2=4|A) * P(A)\n",
    "                 = 4/10 * 3/10 * 1/2\n",
    "                 = 12/200\n",
    "                 = 3/50\n",
    "                 \n",
    "For Class B:\n",
    "\n",
    "P(B|X1=3 , X2=4) = P(X1=3|B) * P(X2=4|B) * P(B)\n",
    "                 = 1/9 * 3/9 * 1/2\n",
    "                 = 3/162\n",
    "                 = 1/54\n",
    "                 \n",
    "Comparing the two probabilities:\n",
    "\n",
    "Since P(A|X1=3 , X2=4) > P(B|X1=3 , X2=4), Naive Bayes would predict that the new instance belongs to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15b662-b0b2-478e-bc8a-aeed63c698f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
